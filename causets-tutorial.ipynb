{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial\n",
    "\n",
    "We will build a Deep Neural Network to classify sprinklings of various discrete spacetimes (causal sets). We will use Google's TensorFlow (https://www.tensorflow.org) to build the DNN, and then use data that I will provide to train and test the DNN.\n",
    "\n",
    "Acknowledgements: I'd like to thank Will Cunningham for most of the data parsing code and help with much of the code in general. \n",
    "\n",
    "Comment: If you find that some of the steps taken in this tutorial are not clear I'd recommend you try to print the output of the statements that are unclear, so that you can see exactly what is going on. I have left a few (commented) print statements here and there which you can try to uncomment and execute should you be interested in seeing what is going on. This is especially useful when it comes to seeing how the data is structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "You can find the data, together with this notebook and any other material that is relevant for the tutorial, here:\n",
    "\n",
    "https://github.com/dbenincasa/ml-tutorial.git\n",
    "\n",
    "To be able to run the code below you'll need to download the data on your machine and modify the 'basedir' variable to point to the folder containing the data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries\n",
    "\n",
    "The first step is to install tensorflow. You can follow the instructions in the link above. Should you have any problems with the installation we can try to fix it during our discussion (with Jef's help!). \n",
    "\n",
    "As well as tensorflow we will need the numpy and pandas libraries. The former should already be part of your python installation, while if you don't have the latter you can install it using pip: \n",
    "\n",
    "$pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and parse data\n",
    "\n",
    "We begin by importing and parsing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the path in the basedir variable with your own local path\n",
    "basedir = '/Users/dbenincasa/Documents/machine-learning/ml-tutorial/'\n",
    "files = pd.read_table(basedir + 'training_files.dat', delim_whitespace = True, \n",
    "                      comment = '#', names = ['label', 'filename'])\n",
    "#print(files, '\\n')\n",
    "\n",
    "tp = [pd.read_table(file, delim_whitespace = True, names = range(100), dtype = int) \n",
    "      for file in files['filename'].values]\n",
    "# a table of size 10000x100. Each row corresponds to one training data point. The columns\n",
    "# correspong to abundances of order intervals N0, N1, ..., N99.\n",
    "#print(tp[0])\n",
    "\n",
    "ltp = [len(p) for p in tp]\n",
    "# a list of sizes of training sets for each class.\n",
    "#print(ltp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas concat method to concatenate the data for the three classes together\n",
    "training_predictors = pd.concat(tp).reset_index(drop=True)\n",
    "\n",
    "# We'll may need the following should we decide to train with causets of size > 100. As is is\n",
    "# it doesn't really do anything\n",
    "training_predictors = training_predictors[training_predictors.columns[:100]]#[:1000:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has some very nice methods that allow us to easily display information about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_predictors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training classes\n",
    "\n",
    "The class label for each training data must be given as a vector of dimension equal to the number of output neurons (i.e. the number of classes), with a 1 for the correct class and zero everywhere else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = []\n",
    "total = 0\n",
    "training_classes = pd.DataFrame()\n",
    "for i, label in enumerate(files['label'].values):\n",
    "    if not -label in training_classes.columns:\n",
    "        training_classes[-label] = pd.concat([pd.Series([0 for x in range(0, total)]),\n",
    "                                             pd.Series([1 for x in range(total, total + ltp[i])]),\n",
    "                                             pd.Series([0 for x in range(ltp[i], len(training_predictors))])]).reset_index(drop=True)\n",
    "    else:\n",
    "        training_classes[-label][total:(total+ltp[i])] = 1\n",
    "    total += ltp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffle data. This step is not needed if we use the full data set for training, but we will actually split the data into a training set and a validation set. The latter is usually used to tune hyperparameters. We will use it primarily to do a preliminary test of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append training classes columns to end of training predictors matrix, and randomly select 100% of the data: \n",
    "# sample(frac=1).\n",
    "shuffled_data = pd.concat([training_predictors, training_classes], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "training_predictors = shuffled_data[shuffled_data.columns[:100]]\n",
    "training_classes = shuffled_data[shuffled_data.columns[-3:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Recap\n",
    "\n",
    "Our training data is composed of pairs (x_i, y_i), i = 1,...,10000, where the training inputs x_i, each contain 100 features corresponding to 100 types of order intervals, and the labels y_i determine which class each input corresponds to. Note that each label y_i is a \"one-hot\" vector, e.g. an input corresponding to d = 2 has y = (1, 0, 0). This representation of the labels is needed for tensorflow to work.\n",
    "\n",
    "All this data has been neatly put into a pandas dataframe which is very easy to manipulate. Although we're not going to analyse the data more thoroughly in this tutorial, the pandas dataframe is very effective at doing data analysis should you wish to do so (we may see a few examples of its capabilities at a later stage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "WARNING: A new version of tensorflow is out (v1.6) which has higher-level APIs that simplify things considerably. In particular, many of the functions that we'll use in this tutorial can be entirely bypassed in the current version. TF current tutorial says \"We recommend using the higher level APIs to build models when possible\", but because my code was written with the older version I'll stick with the lower level APIs for now, and will probably upgrade at a later date.\n",
    "\n",
    "In this section I will explain some of tensorflow's APIs (the ones I use) as I build the network. As such there will be many aspects of tensorflow that I won't talk about (and don't know about!) but that you can read about in the official documentation. \n",
    "\n",
    "The central unit of TF is the *tensor*. Tensors are specified by their *rank* (the number of dimensions), e.g. rank 0 = scalar, rank 1 = vector, rank 2 = matrix etc., and *shape*  (the length of the array along each dimension).\n",
    "\n",
    "A computational graph is a series of TF operations arranged in a graph that is composed of two types of objects:\n",
    "\n",
    "1) Operations -- the graph's nodes\n",
    "\n",
    "2) Tensors -- the graph's edges. These represent the values that flow through the graph.\n",
    "\n",
    "To evaluate tensors, instantiate a tf.Session object, informally known as a session. A session encapsulates the state of the TF runtime, and runs TF operations. If a tf.Graph is like a .py file, a tf.Session is like the python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN architecture:\n",
    "\n",
    "We will define the architecture of our DNN using the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of inputs (features/predictors/factors) for each causal set, i.e. interval abundances\n",
    "D = 3   # Number of outputs (classes), i.e. dimensions (2,3,4)\n",
    "H = [72, 48, 24]   # The number and sizes of the hidden layers. Here we have three hidden layers with 72, 48, \n",
    "                   # and 24 neurons, respectively\n",
    "L = [N] + H + [D]  # All the layer sizes together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TF graph can be parameterized to accept external inputs, such as our abundances of order intervals. These known as *placeholders*. A placeholder is a promise to provide a value later, like a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = tf.placeholder(\"float\", [None, N], name=\"predictors\") # placeholder for inputs\n",
    "classes = tf.placeholder(\"float\", [None, D], name=\"classes\") # placeholder for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases are defined as *variables* in tensorflow. Basically variables can get updated by tensorflow's operations. \n",
    "\n",
    "We now define two functions. \n",
    "\n",
    "The first takes as inputs the number of inputs and outputs of a given layer and initialises the weights and biases associated to that layer at random according to a normal distribution. E.g. our first hidden layer takes in 100 inputs and spits out 72 outputs, so the weight matrix associated to that layer is 100x72 and the bias vector is 72x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb(num_inputs, num_outputs):\n",
    "        weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev = 0.0001))\n",
    "        biases = tf.Variable(tf.ones([num_outputs]))\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function returns the output of each layer given some input. We will use a rectified linear unit (ReLU) as the activation function for our hidden neurons, while the output layer will be a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hl(data, weights, biases):\n",
    "        layers = []\n",
    "        for i in range(len(weights) - 1):\n",
    "                layers.append(tf.nn.relu(tf.matmul(data, weights[i]) + biases[i]))\n",
    "                data = layers[i]\n",
    "        model = tf.nn.softmax(tf.matmul(layers[-1], weights[-1]) + biases[-1], name=\"model\")\n",
    "        return model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the weight matrix and bias for each layer:\n",
    "W = []\n",
    "b = []\n",
    "for i in range(len(L) - 1):\n",
    "        w0, b0 = get_wb(L[i], L[i+1])\n",
    "        W.append(w0)\n",
    "        b.append(b0)\n",
    "\n",
    "# Check that the shape is correct:\n",
    "#W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the above weights and biases to our network.\n",
    "model, _ = get_hl(predictors, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of cost function\n",
    "\n",
    "As our cost function we'll use cross-entropy. \n",
    "\n",
    "ASIDE: Since the true p.m.f. of each training example is concentrated on the correct class, e.g. p = [0,1,0], the cross entropy cost function can also be seen as the log-likelihood. Hence, minimization of cross-entropy is equivalent to MLE. Also note that minimizing cross entropy is equivalent to minimizing KL divergence.\n",
    "\n",
    "Try experimenting with different kinds of cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_sum(classes * tf.log(tf.clip_by_value(model, 1e-10, 1.0)), name=\"cost\")\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(classes * tf.log(model), reduction_indices=[1]), name = \"cost\")\n",
    "# cost = -tf.reduce_sum(classes * tf.square(tf.clip_by_value(model, 1e-10, 1.0)), name=\"cost\")\n",
    "# cost = tf.reduce_sum(tf.square(model - classes)) / 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training algorithm\n",
    "\n",
    "I've found that the adam optimizer coverges faster than standard SGD. You should still experiment with different optimizers to see what happens\n",
    "\n",
    "The optimizer usually needs a learning rate parameter which we also define below. The adam optimizer comes with a default learning rate that works very well, but one can still feed it a different value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "#op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "We now have all the necessary ingredients to train our network. Before we do so let us first define accuracy functions that we can evaluate during training to see how well the training process is doing as it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(classes, 1), name=\"correct_prediction\")\n",
    "# argmax returns the index with the largest value. If this is equal to the index of the correct class\n",
    "# then we have a correct prediction\n",
    "\n",
    "# accuracy is the percentage of correct predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DNN\n",
    "\n",
    "To train the DNN we must first initialize the tensorflow session as well as all global variables. This is done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF session and global variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation split\n",
    "\n",
    "We will split our training data into a training set and a validation set. The validation set should be used to tune the model's hyperparameters, e.g. network architecture, learning_rate, batch size, etc. For now you can simply think of it as our test set which we use to evaluate how well our model generalises (i.e. how well it does when faced with data that it's never seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 80% of the data will be for training\n",
    "# and the last 20% for testing\n",
    "training_size = int(len(training_predictors) * 0.8)\n",
    "test_size = len(training_predictors) - training_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, recall that to speed up training we use mini-batches, rather than the full training data, to compute the gradient. So we define the mini-batch size as well as the number of training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-BATCH SIZE\n",
    "B = 1000\n",
    "# NUMBER OF TRAINING EPOCHS\n",
    "T = 1000\n",
    "# This variable determines every how many steps we evaluate the model's accuracy and cost.\n",
    "prnt = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \t Cost \t\t Accuracy\n",
      "-----------------------------------\n",
      "100 253.04832 0.99916667\n",
      "200 185.0933 0.99929166\n",
      "300 132.28088 0.999375\n",
      "400 92.91496 0.99954164\n",
      "500 64.45954 0.99954164\n",
      "600 44.355186 0.9995\n",
      "700 30.359287 0.9994583\n",
      "800 20.708824 0.9995\n",
      "900 14.092503 0.9994583\n",
      "1000 9.578306 0.9994583\n",
      "\n",
      "Accuracy: 0.999\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [`labels` out of bound] [Condition x < y did not hold element-wise:x (confusion_matrix/control_dependency:0) = ] [1 3 2...] [y (confusion_matrix/Cast_2:0) = ] [3]\n\t [[Node: confusion_matrix/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_INT64], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_1, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_1-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_3, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_2-0)]]\n\nCaused by op 'confusion_matrix/assert_less/Assert/AssertGuard/Assert', defined at:\n  File \"/Users/dbenincasa/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dbenincasa/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-2ca13799dc6b>\", line 33, in <module>\n    confusion = tf.confusion_matrix(labels=p, predictions=m[1], num_classes= D)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py\", line 176, in confusion_matrix\n    labels, num_classes_int64, message='`labels` out of bound')],\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 403, in assert_less\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 134, in Assert\n    condition, no_op, true_assert, name=\"AssertGuard\")\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\n    original_result = fn()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 132, in true_assert\n    condition, data, summarize, name=\"Assert\")\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 47, in _assert\n    name=name)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): assertion failed: [`labels` out of bound] [Condition x < y did not hold element-wise:x (confusion_matrix/control_dependency:0) = ] [1 3 2...] [y (confusion_matrix/Cast_2:0) = ] [3]\n\t [[Node: confusion_matrix/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_INT64], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_1, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_1-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_3, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_2-0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [`labels` out of bound] [Condition x < y did not hold element-wise:x (confusion_matrix/control_dependency:0) = ] [1 3 2...] [y (confusion_matrix/Cast_2:0) = ] [3]\n\t [[Node: confusion_matrix/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_INT64], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_1, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_1-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_3, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_2-0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2ca13799dc6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [`labels` out of bound] [Condition x < y did not hold element-wise:x (confusion_matrix/control_dependency:0) = ] [1 3 2...] [y (confusion_matrix/Cast_2:0) = ] [3]\n\t [[Node: confusion_matrix/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_INT64], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_1, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_1-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_3, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_2-0)]]\n\nCaused by op 'confusion_matrix/assert_less/Assert/AssertGuard/Assert', defined at:\n  File \"/Users/dbenincasa/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dbenincasa/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-2ca13799dc6b>\", line 33, in <module>\n    confusion = tf.confusion_matrix(labels=p, predictions=m[1], num_classes= D)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py\", line 176, in confusion_matrix\n    labels, num_classes_int64, message='`labels` out of bound')],\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 403, in assert_less\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 134, in Assert\n    condition, no_op, true_assert, name=\"AssertGuard\")\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\n    original_result = fn()\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 132, in true_assert\n    condition, data, summarize, name=\"Assert\")\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 47, in _assert\n    name=name)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): assertion failed: [`labels` out of bound] [Condition x < y did not hold element-wise:x (confusion_matrix/control_dependency:0) = ] [1 3 2...] [y (confusion_matrix/Cast_2:0) = ] [3]\n\t [[Node: confusion_matrix/assert_less/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_INT64, DT_STRING, DT_INT64], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_1, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_1-0, confusion_matrix/assert_less/Assert/AssertGuard/Assert/data_3, ConstantFolding/confusion_matrix/assert_less/Assert/AssertGuard/Assert/Switch_2-0)]]\n"
     ]
    }
   ],
   "source": [
    "# Train the DNN\n",
    "print('Epoch \\t Cost \\t\\t Accuracy')\n",
    "print('-----------------------------------')\n",
    "if(B > 0):\n",
    "        for i in range(1, T + 1):\n",
    "            for j in range(0, training_size, B):\n",
    "                    batch_predictors = training_predictors[j:(j+B)]\n",
    "                    batch_classes = training_classes[j:(j+B)]\n",
    "                    _, curr_model, curr_cost = sess.run([op, model, cost], \n",
    "                                                        feed_dict={predictors: batch_predictors, classes: batch_classes})\n",
    "            if i % prnt == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict = {predictors: training_predictors[:training_size],\n",
    "                                                      classes: training_classes[:training_size]})\n",
    "                    print(i, curr_cost, acc)\n",
    "else:\n",
    "        for i in range(1, T + 1):\n",
    "                _, curr_model, curr_cost = sess.run([op, model, cost],\n",
    "                feed_dict={predictors: training_predictors[:training_size],\n",
    "                classes: training_classes[:training_size]})\n",
    "                if i % prnt == 0:\n",
    "                        print(i, curr_cost, sess.run(accuracy,\n",
    "                              feed_dict={predictors:training_predictors[:training_size],\n",
    "                                         classes: training_classes[:training_size]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on validation set\n",
    "\n",
    "Having trained our model with the given hyperparameters we can now evaluate it on the validation set. The model already works very well, but normally one would use this validation set to tune the hyperparameters and then test the final model on some other, never seen, data set.\n",
    "\n",
    "The confusion matrix is a way to efficiently analyse the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.999\n",
      "[[2066    0    0]\n",
      " [   0 1945    3]\n",
      " [   0    3 1983]]\n"
     ]
    }
   ],
   "source": [
    "#Finally let us evaluate the performance of our model on the remaining 20% of the data\n",
    "if test_size > 0:\n",
    "        feed_dict = {predictors: training_predictors[-test_size:],\n",
    "                     classes: training_classes[-test_size:]}\n",
    "        print('\\nAccuracy:', sess.run(accuracy, feed_dict))\n",
    "        m = sess.run([op, tf.argmax(model, 1)], feed_dict)\n",
    "        cols=training_classes.columns\n",
    "        #print(cols)\n",
    "        bt=training_classes[-test_size:].apply(lambda x : x > 0)\n",
    "        #print(bt)\n",
    "        p = bt.apply(lambda x : list(abs(cols[x.values])-2)[0], axis=1)\n",
    "        #print(p)\n",
    "        confusion = tf.confusion_matrix(labels=p, predictions=m[1], num_classes= D)\n",
    "        with sess.as_default():\n",
    "                print(confusion.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model and quit the session\n",
    "\n",
    "We can now save the model just trained so that we can use it outside of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dbenincasa/Documents/machine-learning/ml-tutorial/causets'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save everything\n",
    "saver = tf.train.Saver()\n",
    "# Save the model\n",
    "saver.save(sess, basedir + 'causets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can quit the sessio by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't needed here but would be had we been running this code in a python script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
