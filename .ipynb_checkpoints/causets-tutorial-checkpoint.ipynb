{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial\n",
    "\n",
    "We will build a Deep Neural Network to classify sprinklings of various discrete spacetimes (causal sets). We will use Google's TensorFlow (https://www.tensorflow.org) to build the DNN, and then use data that I will provide to train and test the DNN.\n",
    "\n",
    "Acknowledgements: I'd like to thank Will Cunningham for most of the data parsing code and help with much of the code in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries\n",
    "\n",
    "The first step is to install tensorflow. You can follow the instructions in the link above. Should you have any problems with the installation we can try to fix it during our discussion (with Jef's help!). \n",
    "\n",
    "As well as tensorflow we will need the numpy and pandas libraries. The former should already be part of your python installation, while if you don't have the latter you can install it using pip: \n",
    "\n",
    "$pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenincasa/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/dbenincasa/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and parse data\n",
    "\n",
    "We begin by importing and parsing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/dbenincasa/Documents/machine-learning/ml-tutorial/'\n",
    "files = pd.read_table(basedir + 'training_files.dat', delim_whitespace = True, \n",
    "                      comment = '#', names = ['label', 'filename'])\n",
    "#print(files, '\\n')\n",
    "\n",
    "tp = [pd.read_table(file, delim_whitespace = True, names = range(100), dtype = int) \n",
    "      for file in files['filename'].values]\n",
    "# a table of size 10000x100. Each row corresponds to one training data point. The columns\n",
    "# correspong to abundances of order intervals N0, N1, ..., N99.\n",
    "#print(tp[0])\n",
    "\n",
    "ltp = [len(p) for p in tp]\n",
    "#print(ltp)\n",
    "# a list of sizes of training sets for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas concat method to concatenate the data for the three classes together\n",
    "training_predictors = pd.concat(tp).reset_index(drop=True)\n",
    "\n",
    "# We'll need the following later:\n",
    "training_predictors = training_predictors[training_predictors.columns[:100]]#[:1000:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = []\n",
    "total = 0\n",
    "training_classes = pd.DataFrame()\n",
    "for i, label in enumerate(files['label'].values):\n",
    "    if not -label in training_classes.columns:\n",
    "        training_classes[-label] = pd.concat([pd.Series([0 for x in range(0, total)]),\n",
    "                                             pd.Series([1 for x in range(total, total + ltp[i])]),\n",
    "                                             pd.Series([0 for x in range(ltp[i], len(training_predictors))])]).reset_index(drop=True)\n",
    "    else:\n",
    "        training_classes[-label][total:(total+ltp[i])] = 1\n",
    "    total += ltp[i]\n",
    "#print(training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffle data. This step is not needed if we use the full data set for training, but we will actually split the data into a training set and a validation set. The latter is usually used to tune hyperparameters. We will use it primarily to do a preliminary test of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append training classes columns to end of training predictors matrix, and randomly select 100% of the data: \n",
    "# sample(frac=1).\n",
    "shuffled_data = pd.concat([training_predictors, training_classes], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "training_predictors = shuffled_data[shuffled_data.columns[:100]]\n",
    "training_classes = shuffled_data[shuffled_data.columns[-3:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Recap\n",
    "\n",
    "Our training data is composed of pairs (x_i, y_i), i = 1,...,10000, where the training inputs x_i, each contain 100 features corresponding to 100 types of order intervals, and the labels y_i determine which class each input corresponds to. Note that each label y_i is a \"one-hot\" vector, e.g. an input corresponding to d = 2 has y = (1, 0, 0). This representation of the labels is needed for tensorflow to work.\n",
    "\n",
    "All this data has been neatly put into a pandas dataframe which is very easy to manipulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "WARNING: A new version of tensorflow is out (v1.6) which has higher-level APIs that simplify some things considerably. In particular, many of the functions that we'll use in this tutorial can be entirely bypassed in the current version. TF current tutorial says \"We recommend using the higher level APIs to build models when possible\", but because my code was written with the older version I'll stick with the lower level APIs for now, and will probably upgrade at a later date.\n",
    "\n",
    "In this section I will explain some of tensorflow's APIs (the ones I use) as I build the network. As such there will be many aspects of tensorflow that I won't talk about (and don't know about!) but that you can read about in the official documentation. \n",
    "\n",
    "The central unit of tf is the *tensor*. Tensors are specified by their *rank* (the number of dimensions), e.g. rank 0 = scalar, rank 1 = vector, rank 2 = matrix etc., and *shape*  (the length of the array along each dimension).\n",
    "\n",
    "A computational graph is a series of tf operations arranged in a graph that is composed of two types of objects:\n",
    "\n",
    "1) Operations -- the graph's nodes\n",
    "\n",
    "2) Tensors -- the graph's edges. These represent the values that flow through the graph.\n",
    "\n",
    "To evaluate tensors, instantiate a tf.Session object, informally known as a session. A session encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations. If a tf.Graph is like a .py file, a tf.Session is like the python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN architecture:\n",
    "\n",
    "We will define the architecture of our DNN using the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of inputs (features/predictors/factors) for each causal set, i.e. interval abundances\n",
    "D = 3   # Number of outputs (classes), i.e. dimensions (2,3,4)\n",
    "H = [72, 48, 24]   # The number and sizes of the hidden layers. Here we have three hidden layers with 72, 48, \n",
    "                   # and 24 neurons, respectively\n",
    "L = [N] + H + [D]  # All the layer sizes together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tf graph can be parameterized to accept external inputs, such as our abundances of order intervals. These known as *placeholders*. A placeholder is a promise to provide a value later, like a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = tf.placeholder(\"float\", [None, N], name=\"predictors\") # placeholder for inputs\n",
    "classes = tf.placeholder(\"float\", [None, D], name=\"classes\") # placeholder for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases are defined as *variables* in tensorflow. Basically variables can get updated by tensorflow's operations. \n",
    "\n",
    "We now define two functions. \n",
    "\n",
    "The first takes as inputs the number of inputs and outputs of a given layer and initialises the weights and biases associated to that layer at random according to a normal distribution. E.g. our first hidden layer takes in 100 inputs and spits out 72 outputs, so the weight matrix associated to that layer is 100x72 and the bias vector is 72x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb(num_inputs, num_outputs):\n",
    "        weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev = 0.0001))\n",
    "        biases = tf.Variable(tf.ones([num_outputs]))\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function returns the output of each layer given some input. We will use a rectified linear unit (ReLU) as the activation function for our hidden neurons, and our output layer will be a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hl(data, weights, biases):\n",
    "        layers = []\n",
    "        for i in range(len(weights) - 1):\n",
    "                layers.append(tf.nn.relu(tf.matmul(data, weights[i]) + biases[i]))\n",
    "                data = layers[i]\n",
    "        model = tf.nn.softmax(tf.matmul(layers[-1], weights[-1]) + biases[-1], name=\"model\")\n",
    "        return model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the weight matrix and bias for each layer:\n",
    "W = []\n",
    "b = []\n",
    "for i in range(len(L) - 1):\n",
    "        w0, b0 = get_wb(L[i], L[i+1])\n",
    "        W.append(w0)\n",
    "        b.append(b0)\n",
    "\n",
    "# Check that the shape is correct:\n",
    "#W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the above weights and biases to our network.\n",
    "model, _ = get_hl(predictors, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of cost function\n",
    "\n",
    "As our cost function we'll use cross-entropy. \n",
    "\n",
    "ASIDE: Since the true p.m.f. of each training example is concentrated on the correct class, e.g. p = [0,1,0], the cross entropy cost function can also be seen as the log-likelihood. Hence, minimization of cross-entropy is equivalent to MLE. Also note that minimizing cross entropy is equivalent to minimizing KL divergence.\n",
    "\n",
    "Try experimenting with different kinds of cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_sum(classes * tf.log(tf.clip_by_value(model, 1e-10, 1.0)), name=\"cost\")\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(classes * tf.log(model), reduction_indices=[1]), name = \"cost\")\n",
    "# cost = -tf.reduce_sum(classes * tf.square(tf.clip_by_value(model, 1e-10, 1.0)), name=\"cost\")\n",
    "# cost = tf.reduce_sum(tf.square(model - classes)) / 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training algorithm\n",
    "\n",
    "I've found that the adam optimizer coverges faster than standard SGD. You should still experiment with different optimizers to see what happens\n",
    "\n",
    "The optimizer usually needs a learning rate parameter which we also define below. The adam optimizer comes with a default learning rate that works very well, but one can still feed it a different value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "#op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "We now have all the necessary ingredients to train our network. Before we do so let us first define accuracy functions that we can evaluate during training to see how well the training process is doing as it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(classes, 1), name=\"correct_prediction\")\n",
    "# argmax returns the index with the largest value. If this is equal to the index of the correct class\n",
    "# then we have a correct prediction\n",
    "\n",
    "# accuracy is the percentage of correct predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DNN\n",
    "\n",
    "To train the DNN we must first initialize the tensorflow session as well as all global variables. This is done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF session and global variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation split\n",
    "\n",
    "We will split our training data into a training set and a validation set. The validation set should be used to tune the model's hyperparameters, e.g. network architecture, learning_rate, batch size, etc. For now you can simply think of it as our test set which we use to evaluate how well our model generalises (i.e. how well it does when faced with data that it's never seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 80% of the data will be for training\n",
    "# and the last 20% for testing\n",
    "training_size = int(len(training_predictors) * 0.8)\n",
    "test_size = len(training_predictors) - training_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, recall that to speed up training we use mini-batches, rather than the full training data, to compute the gradient. So we define the mini-batch size as well as the number of training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-BATCH SIZE\n",
    "B = 1000\n",
    "# NUMBER OF TRAINING EPOCHS\n",
    "T = 1000\n",
    "# This variable determines every how many steps we evaluate the model's accuracy and cost.\n",
    "prnt = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \t Cost \t\t Accuracy\n",
      "-----------------------------------\n",
      "100 253.04832 0.99916667\n",
      "200 185.0933 0.99929166\n",
      "300 132.28088 0.999375\n",
      "400 92.91496 0.99954164\n",
      "500 64.45954 0.99954164\n"
     ]
    }
   ],
   "source": [
    "# Train the DNN\n",
    "print('Epoch \\t Cost \\t\\t Accuracy')\n",
    "print('-----------------------------------')\n",
    "if(B > 0):\n",
    "        for i in range(1, T + 1):\n",
    "            for j in range(0, training_size, B):\n",
    "                    batch_predictors = training_predictors[j:(j+B)]\n",
    "                    batch_classes = training_classes[j:(j+B)]\n",
    "                    _, curr_model, curr_cost = sess.run([op, model, cost], \n",
    "                                                        feed_dict={predictors: batch_predictors, classes: batch_classes})\n",
    "            if i % prnt == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict = {predictors: training_predictors[:training_size],\n",
    "                                                      classes: training_classes[:training_size]})\n",
    "                    print(i, curr_cost, acc)\n",
    "else:\n",
    "        for i in range(1, T + 1):\n",
    "                _, curr_model, curr_cost = sess.run([op, model, cost],\n",
    "                feed_dict={predictors: training_predictors[:training_size],\n",
    "                classes: training_classes[:training_size]})\n",
    "                if i % prnt == 0:\n",
    "                        print(i, curr_cost, sess.run(accuracy,\n",
    "                              feed_dict={predictors:training_predictors[:training_size],\n",
    "                                         classes: training_classes[:training_size]}))\n",
    "#Finally let us evaluate the performance of our model on the remaining 20% of the data\n",
    "if test_size > 0:\n",
    "        feed_dict = {predictors: training_predictors[-test_size:],\n",
    "                     classes: training_classes[-test_size:]}\n",
    "        print('\\nAccuracy:', sess.run(accuracy, feed_dict))\n",
    "        m = sess.run([op, tf.argmax(model, 1)], feed_dict)\n",
    "        cols=training_classes.columns\n",
    "        bt=training_classes[-test_size:].apply(lambda x : x > 0)\n",
    "        p = bt.apply(lambda x : list(abs(cols[x.values])-1)[0], axis=1)\n",
    "        confusion = tf.confusion_matrix(labels=p, predictions=m[1], num_classes= D)\n",
    "        with sess.as_default():\n",
    "                print(confusion.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
